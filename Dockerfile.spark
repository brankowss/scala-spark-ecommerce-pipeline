# Official Apache Spark image
FROM apache/spark:3.5.1-scala2.12-java11-python3-ubuntu

# The official image already has a 'spark' user. We switch to root only to install sbt.
USER root

# Install sbt (Scala Build Tool) for compiling Scala code.
# This involves adding the official sbt repository first.
RUN apt-get update && \
    apt-get install -y curl gnupg apt-transport-https && \
    curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | gpg --no-default-keyring --keyring gnupg-ring:/etc/apt/trusted.gpg.d/scalasbt-release.gpg --import && \
    chmod 644 /etc/apt/trusted.gpg.d/scalasbt-release.gpg && \
    echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | tee /etc/apt/sources.list.d/sbt.list && \
    apt-get update && \
    apt-get install -y sbt && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Create the home directory for the 'spark' user and give it the correct ownership.
RUN mkdir -p /home/spark && chown -R spark:spark /home/spark

# Create the directory structure our project expects.
RUN mkdir -p /opt/spark/apps && \
    mkdir -p /opt/spark/jars-custom && \
    mkdir -p /opt/spark/reports

# Change ownership of the newly created directories to the existing 'spark' user.
RUN chown -R spark:spark /opt/spark

# Pre-create the directories that sbt needs to write to.
# We give them open permissions (777) to avoid any conflicts with the mounted volume from the host.
RUN mkdir -p /opt/spark/apps/project && chmod -R 777 /opt/spark/apps/project
RUN mkdir -p /opt/spark/apps/target && chmod -R 777 /opt/spark/apps/target

# Switch back to the non-root 'spark' user.
USER spark

# Set the working directory.
WORKDIR /opt/spark


